!pip install unsloth flask flask-ngrok flask-cors pyngrok
!pip uninstall torch
!pip install torch --extra-index-url https://download.pytorch.org/whl/cu116
!pip install torch
from pyngrok import ngrok

# Replace 'your_authtoken' with the token you copied from your ngrok dashboard
ngrok.set_auth_token("--------------------")
from flask import Flask, request, jsonify
from flask_cors import CORS
import torch
from unsloth import FastLanguageModel
from transformers import AutoTokenizer
import os
from pyngrok import ngrok

# Initialisation du serveur Flask
app = Flask(__name__)
CORS(app)

# Configuration du modèle et du tokenizer
device = "cuda" if torch.cuda.is_available() else "cpu"
model_path = "/content/drive/Shareddrives/Pfa/Qwen2.5version7B r=64_model"

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_path,
    max_seq_length=2048,
    load_in_4bit=True,
    dtype=torch.float16,
    device_map={"": "cuda"}  # ou "cpu" si pas de GPU
)

FastLanguageModel.for_inference(model)

# Route pour générer la réponse
@app.route("/generate", methods=["POST"])
def generate_text():
    data = request.get_json()
    prompt = data.get("prompt", "")
    if not prompt:
        return jsonify({"error": "No prompt provided"}), 400

    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(**inputs, max_new_tokens=200, do_sample=False)
    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
    generated = full_output[len(prompt):].strip()

    return jsonify({"response": generated})

# Expose the Flask app to the web using ngrok
public_url = ngrok.connect(5000)
print(f"Flask app is available at: {public_url}")

# Run Flask server
app.run(host="0.0.0.0", port=5000, threaded=True)

